# Titanic_Survival_Prediction_Using_PyCaret

## Project Description
The goal of this project is to find the best classification model for predicting survival on the Titanic using the PyCaret library. The project involves testing various classification models, applying cross-validation and fine-tuning, and ultimately selecting the best-performing model. Additionally, an ensemble model was created by combining four different models, although it did not achieve high accuracy.

## Dataset Description
The dataset used in this project is the well-known Titanic dataset, which contains information on the passengers of the Titanic, including whether they survived or not. The dataset can be used for binary classification tasks to predict passenger survival based on various features.

## Dataset Source
The dataset used in this project is available on Kaggle: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)

## Project Goals and Objectives
The main objective of this project is to identify the best classification model for predicting survival on the Titanic. The project also aims to apply cross-validation and fine-tuning to improve model performance and to explore the effectiveness of ensemble models.

## Tools and Technologies
- **Programming Language:** Python
- **Library:** PyCaret

## Installation and Setup
To run this project, you need to install the PyCaret library.

The project is built on a Jupyter notebook, so you can run it by simply clicking the run button in the notebook environment.

## Project Structure
The project involves the following steps:

## Data Preprocessing: Cleaning and preparing the Titanic dataset for modeling.
Model Training: Using PyCaret to test various classification models and selecting the best one (XGBoost).
Model Evaluation: Applying cross-validation and fine-tuning to the selected model to improve its performance.
Ensemble Modeling: Creating an ensemble model by combining four different models and evaluating its performance.
## Model Saving and Submission: Saving the best model and uploading the submission result to Kaggle.
## Results and Findings
The best performing model was XGBoost, which was selected after applying cross-validation and fine-tuning. An ensemble model was also created by combining four different models, but it did not achieve a high accuracy rate.

## Future Work
Future improvements could include exploring more advanced ensemble techniques, feature engineering, and data augmentation to enhance model performance.

